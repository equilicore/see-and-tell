"""This script is written to save helper functions used by other scripts.
"""

import os 
import typing
from pathlib import Path
from typing import Union
import json
from PIL import Image
import torch
from torchvision.utils import save_image
import numpy as np

RESIZE = 160
HOME = os.getcwd()

from facenet_pytorch import MTCNN, InceptionResnetV1
# global variable used as the default face detection model
FACE_DETECTION = MTCNN(image_size=RESIZE, keep_all=True, post_process=True, )
# global variable used as the default encoder
ENCODER = InceptionResnetV1(pretrained='vggface2').eval()


def get_embeddings(images: list[Union[Path, str]], resize: int=RESIZE, face_detection=None, encoder=None, keep_all:bool=False, return_faces:bool=False, 
                   save_faces:bool=False, save_path: Union[str, Path]=None, file_name:str=None, ) -> list[list[float]]:

    """This function generates an embedding for each provided images. The faces are detected with the face_detection model. While the embeddings are generated by
    the encoder. The function offers the opportunity to save the face images in the local file system.

    Returns:
        nested list: a list of embeddings
    """

    # set the default value for the face_detection argument
    if face_detection is None:
        face_detection = FACE_DETECTION
        # set the behavior of the face detection as desired
        face_detection.keep_all = keep_all  

    if encoder is None:
        encoder = ENCODER

    if save_path:
        # if the path is relative, conisider it relative to the working directory
        if not os.path.isabs(save_path):
            save_path = os.path.join(HOME, save_path)
        # create the save_path is needed
        os.makedirs(save_path, exist_ok=True)

    # the face detection model as well as the encoder does not expect string object
    images = [Image.open(str(img)) if isinstance(img, str) or isinstance(img, Path) else img for img in images]

    # the output of the face detection model would depend on the keep_all parameter

    if keep_all:
        # the model returns a list for each image 
        face_images = []
        for img in images:
            face_images.extend(list(face_detection.forward(img)))

    else:
        face_images = [face_detection.forward(img) for img in images]

    # define a transformation object to resize the face images and convert them to tensor
    # resize_transformation = T.Resize(size=resize)
    transformations = T.Compose([T.Resize(size=resize)])
    print(face_images[0].shape)

    # resize each face image and convert it to a Tensor.
    final_face_images = torch.stack([transformations(img) for img in face_images]).to(torch.float32)

    # save the images if the corresponding argument is set to True
    if save_faces:
        for index, img in enumerate(face_images): # save the images before the resizing (for better visualization)
            if save_path:
                save_image(img, os.path.join(save_path, f'{(file_name) if file_name else "cropped_image_"}_{index}.jpg'))
            else:
                # just save the resulting pictures in
                save_image(img, os.path.join(HOME, f'{(file_name) if file_name else "cropped_image_"}_{index}.jpg'))    
    if return_faces:

        return ENCODER(final_face_images).detach().cpu().numpy().tolist(), final_face_images

    return ENCODER(final_face_images).detach().cpu().numpy().tolist()


from _collections_abc import Sequence
from torchvision import transforms as T
import matplotlib.pyplot as plt


CONFIDENCE_THRESHOLD = 0.6
REPORT_THRESHOLD = 0.25

from .helper_functions  import cosine_similarity


def recognize_faces(image: Union[str, Path, np.array], embeddings: Union[str, Path], possible_classes: Sequence=None,
                    resize:int=RESIZE, face_detection=None, encoder=None, keep_all=True, 
                   confidence_threshold:float=CONFIDENCE_THRESHOLD, report_threshold:float=REPORT_THRESHOLD, 
                   save_faces:bool=False, save_path:str=None, display:bool=False) -> list[list]:

    """This function, given an image and object representing the embeddings, can detect face images by producing bounding boxes and
    classify them according to the given embeddings .

    Returns: a list of a combination of class (string generally) and bounding boxes
        
    """
    
    # initialize the embeddings:
    if isinstance(embeddings, str) or isinstance(embeddings, Path):
        with open(embeddings, 'r') as f:
            embeddings = json.load(f)
    
    # set the values of the possible classes if needed
    if possible_classes is None:
        possible_classes = list(embeddings.keys())

    # extract the needed classes and save them in a dictionary
    embeddings = dict([(p_c, np.asarray(embeddings[p_c])) for p_c in possible_classes])
    
    # set the default value for the face_detection argument
    if face_detection is None: 
        face_detection = FACE_DETECTION 
        face_detection.keep_all = keep_all # the default behavior is to detect all faces.
        
    # set the default value for the encoder argument
    if encoder is None:
        encoder = ENCODER

    # convert the image to a type that we can work with:
    if isinstance(image, str) or isinstance(image, Path):
        image = Image.open(str(image))
    elif isinstance(image, np.array):
        image = Image.fromarray(image)

    # otherwise, (if the image is a tensor), it should be left as it is                
    
    # detect faces and bounding boxes
    bounding_boxes, bb_probs = face_detection.detect(image)
    # extract the actual face images 
    face_images = face_detection.forward(image)

    # filter by probability: only keep instances with high associated probability
    try:
        result = list(map(list, zip(*[(face, bb, p) for face, bb, p in zip(face_images, bounding_boxes, bb_probs) if p >= confidence_threshold])))
        if not result:
            return []
        face_images, bounding_boxes, bb_probs = result
    except TypeError as e:
        print(e)
        # return an empty list as the model did not detect any images.
        return []

    # the results might improve if we convert the image to gray scale
    transformations = T.Compose([T.Resize(size=resize)])

    final_face_images = torch.stack([transformations((img)) for img in  face_images]).to(torch.float32)

    face_embeddings = encoder(final_face_images).detach().cpu().numpy()

    given_embeddings = list(embeddings.items())
    
    if save_faces:
        for i, img in enumerate(face_images):
            if save_path:
                save_image(img, os.path.join(HOME, save_path, f'cropped_image_{i}.jpg'))        
            else:
                save_image(img, os.path.join(HOME, f'cropped_image_{i}.jpg'))        

    if display:
        for f_img, f_emb, bbox in zip(final_face_images, face_embeddings, bounding_boxes): 
            plt.imshow(f_img.detach().permute(1, 2, 0).cpu().numpy())
            plt.axis('off')
            plt.show()
            print(bbox)
            for char, emb in embeddings.items():
                print(char)
                cos = cosine_similarity(emb, f_emb)
                print(np.mean(cos))
        print("#" * 100)
        print("#" * 100)
        
    cos_similarities = [
                        [np.mean(cosine_similarity(emb, fe)) for _, emb in given_embeddings] 
                        for fe in face_embeddings
                       ]
    
    
    # only consider detected face whose highest similarity exceeds the report threshold.
    indices_sim_boxes = sorted([(np.argmax(average_cos_sim), np.max(average_cos_sim), bb ) 
                        for average_cos_sim, bb in zip(cos_similarities, bounding_boxes)
                        if np.max(average_cos_sim) >= report_threshold], 
                        reverse=True, key=lambda x: x[1]) # order in descending order by highest similarity
    
    # the current results potentially contains duplicates: the same class predicted in different boxes of the image
    # this simply does not make sense in our case as the model is mainly working with human faces

    # The current filtering process is as follows:
    # as the list is sorted by the highest average similarity / score, serving the index (in the list) of the first character-index will guarantee
    # to return the occurence of the face with the highest confidence


    used_char_indices, used_indices = set(), set()

    # i: represents the actual index in the best_indices list
    # index: represents the index corresponding to the character / class 
    for i, (index, _ , _) in enumerate(indices_sim_boxes):
        # if the index: (character index) is seen for the first time, save it as well as the corresponding index in the list 
        if index not in used_char_indices:
            used_char_indices.add(index)
            used_indices.add(i)
    
    # the final result will simply the elements whose list-indices belong the set "used_indices"
    filtered_result = [(best_character_index, bbox) for i, (best_character_index, _, bbox) in enumerate(indices_sim_boxes) if i in used_indices]

    return [[given_embeddings[best_character_index][0], bbox] for (best_character_index, bbox) in filtered_result]
    

from typing import Union
import matplotlib.pyplot as plt

def display_similarity(img1: Union[str, Path, np.array], img2: Union[str, Path, np.array], face_detection=None, encoder=None, resize_size:int=RESIZE):

    # get the embeddings as well as the cropped face images

    embeddings, faces = get_embeddings([img1, img2], resize=resize_size, face_detection=face_detection, encoder=encoder, return_faces=True)

    e1, e2 = embeddings
    
    # calculate the similarity between the embeddings
    similarity = cosine_similarity(e1, e2)

    # the images are tensors. Since Pytorch uses the [C, H, W] convention while Matplotlib uses [H, W, C]
    # permuting the axis is needed.

    # display the first image
    fig, ax = plt.subplots(1, 2)
    ax[0].imshow(faces[0].detach().permute(1, 2, 0).cpu().numpy()) 
    ax[0].set_title(f"first image")
    ax[0].axis("off")

    # display the 2nd image
    ax[1].imshow(faces[1].detach().permute(1, 2, 0).cpu().numpy()) 
    ax[1].set_title(f"second image")
    ax[1].axis("off")

    fig.suptitle(f"Cosine Similarity {round(similarity, 4)}", fontsize=16)
    # dont' forget to display the image 
    plt.show()


from .helper_functions import all_images_in_directory
from collections import Counter


def __build_classes_paths(directory: Union[str, Path]):
    all_images = all_images_in_directory(Path(directory)) # make sure to convert the given path to a Path object
    # iterate through the entire 
    classes_paths = Counter()

    for img_path in all_images:
        # extract the class
        img_path = Path(img_path)
        # the class will simply be the name of the parent directory
        img_class = os.path.basename(img_path.parent)
        if img_class not in classes_paths: 
            # create the list if the class was not yet added
            classes_paths[img_class] = [img_path]
        else:
            # append the current 
            classes_paths[img_class].append(img_path)

    return classes_paths


def build_classes_embeddings(directory: Union[str, Path], save_embedding: Union[str, Path], save_faces: Union[str, Path]=None) -> dict:
    """Given a directory coonstruct a embeddings for each of the classes in this directory. Each subdirectory represents a class

    Args:
        directory (Union[str, Path]): The path to the directory in question
        save_embedding (Union[str, Path]): the path to save the resulting dictionary as a json file
        save_faces (Union[str, Path], optional): the path of the directory to save the faces (the origins of the embeddings). Defaults to None.

    Returns:
        dict: _description_
    """
    # extract the classes paths
    classes_paths = __build_classes_paths(directory=directory)

    # each class has the paths to the corresponding images
    # create the counter for the embeddings now
    embeddings = Counter()

    # preprocess the path passed to save_faces
    if save_faces:
        save_faces = save_faces if os.path.isabs(save_faces) else os.path.join(HOME, save_faces)

    for char in classes_paths.keys():
        # the embeddings of one class are computed simultaneously.
        embeddings[char] = get_embeddings(images=classes_paths[char], keep_all=False, save_faces=(save_faces is not None), save_path=save_faces) # keep_all is set to False as the images are assumed to represent a single face
    
    # save the resulting dictionary as a json file.
    save_embedding = save_embedding if os.path.isabs(save_embedding) else os.path.join(HOME, save_embedding)
    # if the saving path is a directory, save the result in a file with a generic name
    save_embedding = os.path.join(save_embedding, 'embeddings.json') if os.path.isdir(save_embedding) else save_embedding 

    # make sure the file is a json file
    assert str(save_embedding)[-5:] == '.json'

    with open(save_embedding, 'w') as f:
        json.dump(embeddings, f, indent=4)
    
    # return the embeddings
    return embeddings

